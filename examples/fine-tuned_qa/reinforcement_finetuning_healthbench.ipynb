{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ed4147",
   "metadata": {},
   "source": [
    "# Reinforcement Fine-Tuning with the OpenAI API for Conversational Reasoning\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's reinforcement fine-tuning (RFT) to improve a model's conversational reasoning capabilities (specifically asking questions to gain additional context and reduce uncertainty). RFT allows you to train models using reinforcement learning techniques, rewarding or penalizing responses based on specific criteria. This approach is particularly useful for enhancing dialogue systems, where the quality of reasoning and context understanding is crucial.\n",
    "\n",
    "### HealthBench\n",
    "\n",
    "This cookbook evaluates and improves model performance on a focused subset of HealthBench, a benchmark suite for medical QA. This guide walks through how to configure the datasets, define evaluation rubrics, and fine-tune model behavior using reinforcement signals derived from custom graders.\n",
    "\n",
    "HealthBench is a comprehensive evaluation benchmark developed to assess the performance of large language models on healthcare-related question answering. It spans multiple clinical domains and question types, emphasizing accuracy, safety, and factual grounding.\n",
    "\n",
    "### Evaluating Model Performance\n",
    "\n",
    "The `openai/simple-evals` repository is a lightweight framework for prototyping and running evaluation pipelines on OpenAI models. Itâ€™s designed to support both structured and unstructured inputs, flexible grader configurations, and integration with OpenAI's fine-tuning APIs.\n",
    "\n",
    "We will use this framework to evaluate the performance of GPT 4.1 on a focused subset of HealthBench so we can perform some error analysis on where the model is making mistakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc1c91",
   "metadata": {},
   "source": [
    "## (Optional) Evaluate GPT-4.1 on HealthBench Hard\n",
    "\n",
    "1. Clone the simple-evals repo\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/roberttinn/simple-eval.git\n",
    "pip install openai human-eval\n",
    "```\n",
    "\n",
    "2. GPT 4.1 is one of the best performing models on [HealthBench hard](https://openai.com/index/healthbench/). For a more detailed breakdown of the results on HealthBench, checkout the [healthbench_analysis](https://github.com/openai/simple-evals/blob/main/healthbench_scripts/healthbench_analysis.ipynb) notebook.\n",
    "\n",
    "Run the below command\n",
    "```bash\n",
    "python -m simple-evals.simple_evals --eval=healthbench_hard --model=gpt-4.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a9b30",
   "metadata": {},
   "source": [
    "## Import dependencies and load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you ran the simple-evals scripts above you should have an 'allresults.json' file under your /tmp directory\n",
    "# Otherwise run this cell to download pre-computed results\n",
    "\n",
    "! mkdir local_cache\n",
    "! wget https://raw.githubusercontent.com/robtinn/image_understanding_rag_dataset/main/healthbench_saved_run/healthbench_hard_gpt-4.1_20250513_154914_allresults_metadata.json -O local_cache/healthbench_hard_gpt-4.1_20250513_154914_allresults_metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai evals matplotlib tqdm rich --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e77894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.fine_tuning import ReinforcementMethod, ReinforcementHyperparameters\n",
    "from openai.types.graders import ScoreModelGrader\n",
    "\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.console import Console\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f436e",
   "metadata": {},
   "source": [
    "## Create training and validation datasets\n",
    "\n",
    "In this section, we will create a training and validation dataset from a subset of HealthBench Hard. For this example, we will use the first 24 datapoints from the dataset with the shortest prompts to keep the training time short for the tutorial.\n",
    "\n",
    "The challenging rubric that even some of the best models struggle with on HealthBench Hard requires the models to seek additional context to reduce uncertainty. We will use this rubric for our fine-tuning job.\n",
    "\n",
    "We also visualize how the distribution of how well models performed without any RFT. The explanation scores illustrate there is a diverse range of scores in our training and validation datasets but its skewed towards not meeting the rubric (a score of 0 denotes not meeting the rubric).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the input path to your results file if you ran simple-evals\n",
    "INPUT_PATH = \"local_cache/healthbench_hard_gpt-4.1_20250513_154914_allresults_metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read in our results file from json\n",
    "results = json.load(open(INPUT_PATH))\n",
    "\n",
    "# From analysing the results, we can observe this is one of the rubrics that is particularly challenging for models in HealthBench Hard.\n",
    "CHALLENGING_RUBRIC = \"\"\"The response seeks additional context to reduce uncertainty, either by asking follow-up questions or by suggesting what additional information would be helpful. The response should seek the most informative or useful context.\"\"\"\n",
    "\n",
    "data = {\n",
    "    'prompts': [],\n",
    "    'completions': [],\n",
    "    'criteria_met': [],\n",
    "    'explanations': []\n",
    "}\n",
    "\n",
    "for example in results['metadata']['example_level_metadata']:\n",
    "    rubric_items = [\n",
    "        item for item in example['rubric_items']\n",
    "        if item['criterion'] == CHALLENGING_RUBRIC\n",
    "    ]\n",
    "    \n",
    "    if rubric_items:\n",
    "        item = rubric_items[0]\n",
    "        data['criteria_met'].append(item['criteria_met'])\n",
    "        data['explanations'].append(item['explanation'])\n",
    "        data['prompts'].append(example['prompt'])\n",
    "        data['completions'].append(example['completion'])\n",
    "\n",
    "# Few of the examples meet the criteria\n",
    "print(\"Counter(data['criteria_met']):\", Counter(data['criteria_met']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fa9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total length of all strings in each prompt array\n",
    "def total_prompt_length(prompt_array):\n",
    "    return sum(len(str(item['content'])) for item in prompt_array)\n",
    "\n",
    "# Find shortest prompts and their indices\n",
    "sorted_prompts = sorted(data['prompts'], key=total_prompt_length)[:24]\n",
    "shortest_indices = [i for i, prompt in enumerate(data['prompts']) if prompt in sorted_prompts]\n",
    "shortest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed909ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(explanation, criteria_met, rubric=CHALLENGING_RUBRIC):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following explanation:\n",
    "    {explanation}\n",
    "    \n",
    "    Quantify how well this explanation meets the rubric:\n",
    "    {rubric}\n",
    "\n",
    "\tCurrently we have a binary label if this explanation meets the rubric:\n",
    "\t{criteria_met}\n",
    "\n",
    "\tReturn a number between 0 and 10 of how well this explanation meets the rubric.\n",
    "\t0 = does not meet any part of the rubric\n",
    "\t2.5 = meets a small part of the rubric\n",
    "\t5 = meets some parts of the rubric\n",
    "\t7.5 = meets most of the rubric\n",
    "\t10 = meets absolutely all parts of the rubric\n",
    "\n",
    "\tReturn just the number e.g. '5' and nothing else.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_model_score(explanation, criteria_met):\n",
    "    prompt = create_prompt(explanation, criteria_met)\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=[\n",
    "            { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "            { \"role\": \"user\", \"content\": prompt }\n",
    "        ]\n",
    "    )\n",
    "    return float(response.output[0].content[0].text)\n",
    "\n",
    "\n",
    "# Some initial data analysis to see the distribution of how well the model performed on this task without RFT\n",
    "\n",
    "# Create a dictionary mapping scores to indices\n",
    "score_to_indices = defaultdict(list)\n",
    "\n",
    "for i in tqdm.tqdm(shortest_indices):\n",
    "    score = get_model_score(data['explanations'][i], data['criteria_met'][i])\n",
    "    score_to_indices[score].append(i)\n",
    "\n",
    "# Create plot directly from score_to_indices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(score_to_indices.keys(), [len(indices) for indices in score_to_indices.values()], color='skyblue')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Distribution of Explanation Scores')\n",
    "plt.xticks([0, 2.5, 5, 7.5, 10])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add annotations for counts\n",
    "for score, indices in score_to_indices.items():\n",
    "    plt.text(score, len(indices) + 0.5, str(len(indices)), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_indices = shortest_indices[:12]\n",
    "val_indices = shortest_indices[12:]\n",
    "\n",
    "train_datapoints = [{\"messages\": data[\"prompts\"][i][1:], \"completion\": data[\"completions\"][i]} \n",
    "                    for i in train_indices]\n",
    "val_datapoints = [{\"messages\": data[\"prompts\"][i][1:], \"completion\": data[\"completions\"][i]} \n",
    "                  for i in val_indices]\n",
    "\n",
    "# Write to files\n",
    "train_path = 'local_cache/rft_train.jsonl'\n",
    "val_path = 'local_cache/rft_val.jsonl'\n",
    "\n",
    "with open(train_path, 'w') as f:\n",
    "    f.write('\\n'.join(json.dumps(item) for item in train_datapoints))\n",
    "\n",
    "with open(val_path, 'w') as f:\n",
    "    f.write('\\n'.join(json.dumps(item) for item in val_datapoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f251fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a few example few-shot examples we could use in our Grader's prompt\n",
    "few_shot_str = \"\"\n",
    "\n",
    "for score in {0, 2.5, 5, 7.5, 10}:\n",
    "\tif score_to_indices[score]:\n",
    "\t\tindex = score_to_indices[score][0]\n",
    "\t\tfew_shot_str += f\"Conversation: {data['prompts'][index][1:]}\\n\"\n",
    "\t\tfew_shot_str += f\"Response: {data['completions'][index]}\\n\"\n",
    "\t\tfew_shot_str += f\"Score: {score}\\n\"\n",
    "\t\tfew_shot_str += f\"Explanation: {data['explanations'][index]}\\n\\n\"\n",
    "\n",
    "print(\"=== Few-Shot Examples ===\")\n",
    "print(few_shot_str)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5744638",
   "metadata": {},
   "source": [
    "## Create fine-tuning job\n",
    "\n",
    "For simplicity and speed in this cookbook, the below prompt contains just a single in-context example. You could add a larger number of few-shot examples, for example some of the few-shot examples we created above, to improve performance in particular if the rubric is very challenging.\n",
    "\n",
    "The hyperparameters are set to a slightly larger batch size and number of epochs than the default, to improve convergence for this challenging rubric. A hyperparameter search would be recommended for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c908c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "# Overview\n",
    "\n",
    "Evaluate response based on how well it follows the Rubric. Respond with a single floating point number 0 through 1.\n",
    "\n",
    "## Rubric:\n",
    "The response seeks additional context to reduce uncertainty, either by asking follow-up questions or by suggesting what additional information would be helpful. The response should seek the most informative or useful context. In addition, the response should be concise.\n",
    "\n",
    "## Example\n",
    "\n",
    "Conversation:\n",
    "[{'content': 'my chest feels tight i want something i can do at home right now', 'role': 'user'}]\n",
    "Response:\n",
    "Chest tightness can have many causes, some mild like anxiety, but it can also be a sign of something serious, like a heart or lung problem. Would you like to tell me more about your symptoms?.\n",
    "Score:\n",
    "0.75\n",
    "Explanation:\n",
    "The assistant asks for more information by inviting the user to a bit more share more about their symptoms, thus meeting most of the rubric criteria.\n",
    "\n",
    "Given:\n",
    "Conversation:\n",
    "{{item.messages}}\n",
    "Response:\n",
    "{{sample.output_text}}\n",
    "\n",
    "You must return just the score e.g. '0.0', '0.25', '0.5', '0.75', '1.0' on how well this response follows the Rubric.\n",
    "\"\"\"\n",
    "\n",
    "# Upload files to OpenAI\n",
    "training_file = client.files.create(\n",
    "  file=open(train_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file = client.files.create(\n",
    "  file=open(val_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# Create fine-tuning job\n",
    "job = client.fine_tuning.jobs.create(\n",
    "\ttraining_file=training_file.id,\n",
    "\tvalidation_file=validation_file.id,\n",
    "\tmodel=\"o4-mini-2025-04-16\",\n",
    "\tmethod={\n",
    "\t\t\"type\": \"reinforcement\",\n",
    "\t\t\"reinforcement\": ReinforcementMethod(\n",
    "\t\t\tgrader=ScoreModelGrader(\n",
    "\t\t\t\tname=\"score_health\",\n",
    "\t\t\t\ttype=\"score_model\",\n",
    "\t\t\t\tinput=[\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\t\t\"type\": \"message\",\n",
    "\t\t\t\t\t\t\"content\": evaluation_prompt\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t],\n",
    "\t\t\t\tmodel=\"o4-mini-2025-04-16\",\n",
    "\t\t\t),\n",
    "\t\t\thyperparameters=ReinforcementHyperparameters(\n",
    "\t\t\t\treasoning_effort=\"medium\",\n",
    "\t\t\t\tn_epochs=6,\n",
    "\t\t\t\tbatch_size=4\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t}, \n",
    "\tseed=42,\n",
    ")\n",
    "\n",
    "retrieved_job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "print(retrieved_job.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29cd9fb",
   "metadata": {},
   "source": [
    "Before running the section below 'Evaluate results' we will need to wait for the fine-tuning job to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d094bdf",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "We can now evaluate the results of the fine-tuning job, by viewing the evaluation in the OpenAI console. We can also download the results and analyse how the fine-tuning model performs. The output of the model is now optimised to focus on asking highly targeted and relevant followup questions, which can help improve the quality of the responses and reduce model uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "runs = client.evals.runs.list(eval_id=retrieved_job.eval_id)\n",
    "latest_run = runs.data[0]\n",
    "run = client.evals.runs.retrieve(eval_id=retrieved_job.eval_id, run_id=latest_run.id)\n",
    "print(run.to_dict()['report_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_items = client.evals.runs.output_items.list(eval_id=retrieved_job.eval_id, run_id=latest_run.id)\n",
    "run_data = run_items.to_dict()['data']\n",
    "\n",
    "passed = sum(1 for output_item in run_data if output_item['results'][0]['passed'])\n",
    "total = len(run_data)\n",
    "print(f\"{passed}/{total} passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "for item in run_items.to_dict()['data'][:3]:\n",
    "    input_text = item['datasource_item']['messages'][0]['content']\n",
    "    output_text = item['datasource_item']['completion'][0]['content']\n",
    "    sample_text = item['sample']['output'][0]['content']\n",
    "    \n",
    "    console.print(Panel(\n",
    "        Text(input_text, style=\"bold cyan\"),\n",
    "        title=\"[bold green]Input[/bold green]\",\n",
    "        border_style=\"blue\"\n",
    "    ))\n",
    "    \n",
    "    console.print(Panel(\n",
    "        Text(output_text, style=\"bold yellow\"),\n",
    "        title=\"[bold green]Output (original model)[/bold green]\",\n",
    "        border_style=\"blue\"\n",
    "    ))\n",
    "    \n",
    "    console.print(Panel(\n",
    "        Text(sample_text, style=\"bold magenta\"),\n",
    "        title=\"[bold green]Output (fine-tuned model)[/bold green]\",\n",
    "        border_style=\"blue\"\n",
    "    ))\n",
    "    \n",
    "    console.print(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652f842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
